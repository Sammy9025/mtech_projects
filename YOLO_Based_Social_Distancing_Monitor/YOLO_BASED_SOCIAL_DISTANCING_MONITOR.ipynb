{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> YOLO BASED SOCIAL DISTANCING VIOLATION MONITOR </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem statement:\n",
    "### To identify whether the human-beings are maintaining social distance norms or not from a real-time or recorded video footage. It will help to spread awareness among the population."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. SUPPORTING FILES**<br>\n",
    "    ***A.\"coco.names\"     :*** https://github.com/pjreddie/darknet/blob/master/data/coco.names<br>\n",
    "    ***B.\"yolov3.cfg\"     :*** https://github.com/pjreddie/darknet/blob/master/cfg/yolov3.cfg <br>\n",
    "    ***C.\"yolov3.weights\" :*** https://drive.google.com/file/d/1tIxUU2cWiK422X77lykuMVlTrH0Ff7rl/view?usp=share_link<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Information:\n",
    "**MODEL USED:** YOLO V3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>----------------------------------------------------------------------------------------------------------</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all required libraries \n",
    "from scipy.spatial import distance as dist\n",
    "import numpy as np\n",
    "import imutils\n",
    "import cv2\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize minimum threshold for object detection\n",
    "MIN_CONF   = 0.40\n",
    "NMS_THRESH = 0.20\n",
    "\n",
    "# define the minimum safe pixel distance \n",
    "MIN_DISTANCE = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels present in COCO: \n",
      " ['person', 'bicycle', 'car', 'motorbike', 'aeroplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'sofa', 'pottedplant', 'bed', 'diningtable', 'toilet', 'tvmonitor', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'] \n",
      "\n",
      "Number of labels :\n",
      " 80 \n",
      "\n",
      "Extracted...\n",
      "['yolo_82', 'yolo_94', 'yolo_106']\n"
     ]
    }
   ],
   "source": [
    "# load the COCO class \n",
    "labelsPath = os.path.sep.join([\"coco.names\"])\n",
    "LABELS = open(labelsPath).read().strip().split(\"\\n\")\n",
    "\n",
    "#print the labes\n",
    "print('Labels present in COCO: \\n',LABELS,'\\n')\n",
    "print('Number of labels :\\n',len(LABELS),'\\n')\n",
    "\n",
    "weightsPath = os.path.sep.join([ \"yolov3.weights\"])\n",
    "configPath  = os.path.sep.join([ \"yolov3.cfg\"])\n",
    "\n",
    "# load the YOLO data trained on COCO dataset \n",
    "print(\"Extracted...\")\n",
    "net = cv2.dnn.readNetFromDarknet(configPath, weightsPath)\n",
    "\n",
    "# determine only the \"output\" layer names\n",
    "layer_names  = net.getLayerNames()\n",
    "output_layer = [layer_names[i-1] for i in net.getUnconnectedOutLayers()]\n",
    "print(output_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming the video clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPS of the current video:  30.0\n",
      "Number of frames in the video:  11784.0\n"
     ]
    }
   ],
   "source": [
    "video_stream = cv2.VideoCapture(\"test_video.mp4\")\n",
    "\n",
    "fps = video_stream.get(cv2.CAP_PROP_FPS)\n",
    "print(\"FPS of the current video: \",fps)\n",
    "\n",
    "num_frames = video_stream.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "print(\"Number of frames in the video: \",num_frames)\n",
    "\n",
    "writer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Euclidean distance \n",
    "def euclidean_dist(p1, p2):\n",
    "    \n",
    "    return ((p1[0] - p2[0]) ** 2 +  (p1[1] - p2[1]) ** 2) ** 0.5\n",
    "\n",
    "def isclose(p1, p2):\n",
    "    \n",
    "    calculated_distance = euclidean_dist(p1, p2)\n",
    "    calib = (p1[1] + p2[1]) / 2\n",
    "\n",
    "    if 0 < calculated_distance < 0.15 * calib:\n",
    "        return 1\n",
    "\n",
    "    elif 0 < calculated_distance < 0.2 * calib:\n",
    "        return 2\n",
    "\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "    \n",
    "height,width=(None,None)\n",
    "q=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the function to detect People"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_people(frame, net, output_layer, personIdx=0):\n",
    "    # dimensions of the frame \n",
    "    (H, W) = frame.shape[:2]\n",
    "    results = []\n",
    "\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1 / 255.0, (416, 416), swapRB=True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    layerOutputs = net.forward(output_layer)\n",
    "\n",
    "    boxes = []\n",
    "    centroids = []\n",
    "    confidences = []\n",
    "\n",
    "    # loop over each of the layer outputs\n",
    "    for output in layerOutputs:\n",
    "        # loop over each of the detections\n",
    "        for detection in output:\n",
    "            \n",
    "            scores = detection[5:]\n",
    "            classID = np.argmax(scores)\n",
    "            confidence = scores[classID]\n",
    "           \n",
    "            if classID == personIdx and confidence > MIN_CONF:\n",
    "                \n",
    "                box = detection[0:4] * np.array([W, H, W, H])\n",
    "                (centerX, centerY, width, height) = box.astype(\"int\")\n",
    "               \n",
    "                x = int(centerX - (width / 2))\n",
    "                y = int(centerY - (height / 2))\n",
    "              \n",
    "                boxes.append([x, y, int(width), int(height)])\n",
    "                centroids.append((centerX, centerY))\n",
    "                confidences.append(float(confidence))\n",
    "\n",
    "    \n",
    "    idxs = cv2.dnn.NMSBoxes(boxes, confidences, MIN_CONF, NMS_THRESH)\n",
    "\n",
    "    # ensure at least one detection exists\n",
    "    if len(idxs) > 0:\n",
    "        # loop over the indexes we are keeping\n",
    "        for i in idxs.flatten():\n",
    "            # extract the bounding box coordinates\n",
    "            (x, y) = (boxes[i][0], boxes[i][1])\n",
    "            (w, h) = (boxes[i][2], boxes[i][3])\n",
    "\n",
    "            r = (confidences[i], (x, y, x + w, y + h), centroids[i])\n",
    "            results.append(r)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the output frames as a video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "display = 1\n",
    "output = \"output_video.avi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    # read the next frame from the file\n",
    "    (grabbed, frame) = video_stream.read()\n",
    "\n",
    "    \n",
    "    if not grabbed:\n",
    "        break\n",
    "\n",
    "    # resize the frame and then detect people (and only people) in it\n",
    "    frame = imutils.resize(frame, width=700)\n",
    "    results = detect_people(frame, net, output_layer, personIdx=LABELS.index(\"person\"))\n",
    "    height,width=frame.shape[:2]\n",
    "\n",
    "    violate = set()\n",
    "\n",
    "   \n",
    "    if len(results) >= 2:\n",
    "        \n",
    "        centroids = np.array([r[2] for r in results])\n",
    "        D = dist.cdist(centroids, centroids, metric=\"euclidean\")\n",
    "\n",
    "       \n",
    "        for i in range(0, D.shape[0]):\n",
    "            for j in range(i + 1, D.shape[1]):\n",
    "              \n",
    "                if D[i, j] < MIN_DISTANCE:\n",
    "                   \n",
    "                    violate.add(i)\n",
    "                    violate.add(j)\n",
    "\n",
    "    # loop over the results\n",
    "    for (i, (prob, bbox, centroid)) in enumerate(results):\n",
    "       \n",
    "        (startX, startY, endX, endY) = bbox\n",
    "        (cX, cY) = centroid\n",
    "        color = (0, 255, 0)\n",
    "       \n",
    "        if i in violate:\n",
    "            color = (0, 0, 255)\n",
    "        \n",
    "        cv2.rectangle(frame, (startX, startY), (endX, endY), color, 2)\n",
    "        cv2.putText(frame, str(round(prob * 100))+\"%\", (startX - 5, startY - 5), cv2.FONT_HERSHEY_PLAIN, 1, (255, 255, 0), 1)\n",
    "        cv2.circle(frame, (cX, cY), 5, color, 1)\n",
    "        \n",
    "\n",
    "    # draw the total number of social distancing violations on the output frame\n",
    "    text = \"Social Distancing Violations Detected: {}\".format(len(violate))\n",
    "    cv2.putText(frame, text, (10, frame.shape[0] - 25), cv2.FONT_HERSHEY_SIMPLEX, 0.85, (0, 255, 255), 2)\n",
    "    cv2.putText(frame,'People Detected:{}'.format(len(results)), (10, height - 75),cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)\n",
    "    \n",
    "\n",
    "    if display > 0:\n",
    "        # show the output frame\n",
    "        cv2.imshow(\"Frame\", frame)\n",
    "            \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "   \n",
    "    if output != \"\" and writer is None:\n",
    "        # initialize our video writer\n",
    "        fourcc = cv2.VideoWriter_fourcc(*\"MJPG\")\n",
    "        writer = cv2.VideoWriter(output, fourcc, 25, (frame.shape[1], frame.shape[0]), True)\n",
    "\n",
    "    if writer is not None:\n",
    "        writer.write(frame)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>-------------------------------------------------END------------------------------------------------</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
